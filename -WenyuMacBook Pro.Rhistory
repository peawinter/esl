par(mfrow = c(1, 2), mar = c(4.5, 4.5, 1, 1), oma = c(0, 0, 4, 0))
plot(age, wage, xlim = agelims, cex = .5, col = "dardgrey")
plot(age, wage, xlim = agelims, cex = .5, col = "darkgrey")
plot(age, wage, xlim = agelims, cex = .5, col = "darkgrey")
par(mfrow = c(1, 2), mar = c(4.5, 4.5, 1, 1), oma = c(0, 0, 4, 0))
plot(age, wage, xlim = agelims, cex = .5, col = "darkgrey")
title("Degree-4 polynomial", outer = T)
lines(age.grid, preds$fit, lwd = 2, col = "blue")
matlines(age.grid, se.bands, lwd = 1, col = "blue", lty = 3)
preds2 <- predict(fit2, newdata = list(age = age.grid), se = T)
preds2 <- predict(fit2b, newdata = list(age = age.grid), se = T)
max(abs(preds$fit - preds$fit))
max(abs(preds$fit - preds2$fit))
fit.1 <- lm(wage ~ age, data = Wage)
fit.2 <- lm(wage ~ poly(age, 2), data = Wage)
fit.3 <- lm(wage ~ poly(age, 3), data = Wage)
fit.4 <- lm(wage ~ poly(age, 4), data = Wage)
fit.5 <- lm(wage ~ poly(age, 5), data = Wage)
anova(fit.1, fit.2, fit.3, fit.4, fit.5)
coef(summary(fit.5))
age
I(age ^ 2)
?I
fit <- glm(I(wage > 250) ~ poly(age, 4), data = Wage, family = binomial)
preds <- predict(fit, newdata = list(age = age.gird), se = T)
preds <- predict(fit, newdata = list(age = age.grid), se = T)
preds
plot(preds ~ age.grid)
plot(age.grid, preds)
plot(age.grid, preds$fit)
pfit <- exp(preds$fit) / (1 + exp(preds$fit))
pfit <- exp(preds$fit) / (1 + exp(preds$fit))
se.bands.logit <- cbind(preds$fit + 2 * preds$se.fit,
preds$fit - 2 * preds$se.fit)
preds <- predict(fit, newdata = list(age = age.grid), type = "response",
se = T)
plot(age.grid, preds$fit)
plot(age, I(wage > 250), xlim = agelims,
type = "n", ylim = c(0, 0.2))
plot(age, I(wage > 250), xlim = agelims,
type = "n", ylim = c(0, 0.2))
points(jitter(age), I((wage > 250)/5), cex = .5, pch = "|",
col = "darkgrey")
?jitter
lines(age.grid, pfit, lwd = 2, col = "blue")
matlines(age.grid, se.bands, lwd = 1, col = "blue", lty = 3)
se.bands
pfit
se.bands
table(cut(age, 4))
fit <- lm(wage ~ cut(age, 4), data = Wage)
coef(summary(fit))
cut(age, 4)
library(splines)
library(splines)
fit <- lm(wage ~ bs(age, knots = c(25, 40, 60)), data = Wage)
pred <- predict(fit, newdata = list(age = age.grid), se = T)
plot(age.wage, col = "grey")
plot(age, wage, col = "grey")
lines(age.grid, pred$fit, lwd = 2)
lines(age.grid, pred$fit + 2 * pred$se, lty = "dashed")
lines(age.grid, pred$fit - 2 * pred$se, lty = "dashed")
dim(bs(age, knots = c(25, 40, 60)))
bs(age, knots = c(25, 40, 60))
fit2 <- lm(wage ~ ns(age, df = 4), data = Wage)
pred2 <- predict(fit2, newdata = list(age = age.grid), se = T)
lines(age.grid, pred2$fit, col = "red", lwd = 2)
plot(age, wage, xlim = agelims, cex = .5, col = "darkgrey")
title("smoothing spline")
fit <- smooth.spline(age, wage, df = 16)
fit2 <- smooth.spline(age, wage, cv = T)
fit2$df
lines(fit, col = "red", lwd = 2)
lines(ft2, col = "blue", lwd = 2)
lines(fit2, col = "blue", lwd = 2)
legend("topright", legend = c("16 DF", "6.8 DF"))
?ns
gam1 <- lm(wage ~ ns(year, 4) + ns(age, 5) + education, data = Wage)
library(gam)
install.packages("gam")
library(gam)
gam.m3 <- gam(wage ~ s(year, 4) + s(age, 5) + education, data = Wage)
par(mfrow = c(1, 3))
plot(gam.m3, se = T, col = "blue")
plot(gam1, se = T, col = "red")
par(mfrow = c(1, 1))
library(tree)
install.packages("tree")
library(tree)
attach(Carseats)
library(ISLR)
attach(Carseats)
High = ifelse(Sales <= 8, "No", "Yes")
Carseats = data.frame(Carseats, High)
head(Carseats)
tree.Carseats <- tree(High ~. - Sales, Carseats)
summary(tree.Carseats)
plot(tree.carseats)
plot(tree.Carseats)
text(tree.Carseats, pretty = 0)
tree.Carseats
set.seed(2)
train <- sample(1:nrow(Carseats), 200)
Carseats.test <- Carseats[-train, ]
High.test <- High[train]
tree.carseats <- tree(High ~. - Sales, Carseats, subset = train)
tree.carseats <- tree(High ~. - Sales, Carseats, subset = train)
tree.pred <- predict(tree.carseats, Carseats.test, type = "class")
table(tree.pred, High.test)
cv.carseats <- cv.tree(tree.carseats, FUN = prune.misclass)
names(cv.carseats)
cv.carseats
par(mfrow = c(1, 2))
plot(cv.carseats$size, cv.carseats$dev, type = "b")
par(mfrow = c(1, 2))
plot(cv.carseats$size, cv.carseats$dev, type = "b")
par(mfrow = c(1, 2))
plot(cv.carseats$size, cv.carseats$dev, type = "b")
plot(cv.carseats$k, cv.carseats$dev, type = "b")
prune.carseats <- prune.misclass(tree.carseats, best = 9)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
tree.pred <- predict(prune.carseats, Carseats.test, type = "class")
table(tree.pred, High.test)
cv.carseats
prune.carseats <- prune.misclass(tree.carseats, best = 15)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
tree.pred <- predict(prune.carseats, Carseats.test, type = "class")
table(tree.pred, High.test)
library(MASS)
set.seed(1)
train <- sample(1:nrow(Boston), nrow(Boston)/2)
tree.boston <- tree(medv ~., Boston, subset = train)
summary(tree.boston)
plot(tree.boston)
text(tree.boston, pretty = 0)
cv.boston <- cv.tree(tree.boston)
plot(cv.boston$size, cv.boston$dev, type = 'b')
prune.boston <- prune.tree(tree.boston, best = 5)
plot(prune.boston)
test(prune.boston, pretty = 0)
text(prune.boston, pretty = 0)
yhat <- predict(tree.boston, newdata = Boston[-train, ])
boston.test <- Boston[-train, "medv"]
plot(yhat, boston.test)
abline(0, 1)
mean((yhat - boston.test) ^ 2)
intall.packages("randomForest")
install.packages("randomForest")
set.seed(1)
bag.boston <- randomForest(medv ~., data = Boston, subset = train,
mtry = 13, importance = TRUE)
bag.boston
library(randomForest)
bag.boston <- randomForest(medv ~., data = Boston, subset = train,
mtry = 13, importance = TRUE)
bag.boston
yhat.bag <- predict(bag.boston, newdata = Boston[-train, ])
plot(yhat.bag, boston.test)
abline(0, 1)
mean((yhat.bag - boston.test) ^ 2)
bag.boston <- randomForest(medv ~ ., data = Boston, subset = train,
mtry = 13, ntree = 25)
yhat.bag <- predict(bag.boston, newdata = Boston[-train, ])
mean((yhat.bag - boston.test) ^ 2)
set.seed(1)
rf.boston <- randomForest(medv ~., data = Boston, subset = train,
mtry = 6, importance = T)
yhat.rf <- predict(rf.boston, newdata = Boston[-train, ])
mean((yhat.rf - boston.test) ^ 2)
importance(rf.boston)
varImpPlot(rf.boston)
library(gbm)
install.packages("gbm")
library(gbm)
set.seed(1)
boost.boston <- gbm(medv ~., data = Boston[train, ], distribution =
"gaussian", n.trees = 5000, interaction.depth = 4)
summary(boost.boston)
plot(boost.boston, i = "rm")
plot(boost.boston, i = "lstat")
yhat.boost <- predict(boost.boston, newdata = Boston[-train, ],
n.trees = 5000)
mean((yhat.boost - boston.test) ^ 2)
set.seed(1)
x <- matrix(rnorm(2 * 20), ncol = 2)
y <- c(rep(-1, 10), rep(1, 10))
x[y == 1, ] <- x[y == 1, ] + 1
plot(x, col = (3 - y))
data <- data.frame(x = x, y = as.factor(y))
library(e1071)
svmfit <- svm(y ~., data = dat, kernal = "linear", cost = 10,
scale = F)
dat <- data.frame(x = x, y = as.factor(y))
svmfit <- svm(y ~., data = dat, kernal = "linear", cost = 10,
scale = F)
plot(svmfit, dat)
svmfit <- svm(y ~., data = dat, kernal = "linear", cost = 10,
scale = F)
plot(svmfit, dat)
svmfit$index
summary(svmfit)
### Chapter 9
set.seed(1)
x <- matrix(rnorm(20 * 2), ncol = 2)
y <- c(rep(-1, 10), rep(1, 10))
x[y == 1, ] <- x[y == 1, ] + 1
plot(x, col = (3 - y))
dat <- data.frame(x = x, y = as.factor(y))
library(e1071)
svmfit <- svm(y ~., data = dat, kernal = "linear", cost = 10,
scale = F)
plot(svmfit, dat)
svmfit$index
summary(svmfit)
svmfit <- svm(y ~., data = dat, kernal = "linear", cost = .1,
scale = F)
plot(svmfit, dat)
svmfit$index
summary(svmfit)
set.seed(1)
tune.out <- tune(svm, y ~ ., data = dat, kernel = "linear",
ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tune.out)
bestmod = tune.out$best.model
summary(bestmod)
xtest = matrix(rnorm(20 * 2), ncol = 2)
ytest = sample(c(1, -1), 20, rep = T)
xtest[ytest == 1, ] = xtest[ytest == 1, ] + 1
testdat = data.frame(x = xtest, y = as.factor(ytest))
ypred <- predict(bestmod, testdat)
table(predict = ypred, truth = testdat$y)
bestmod
x[y == 1, ] = x[y == 1, ] + 0.5
plot(x, col = (y + 5)/2, pch = 19)
dat <- data.frame(x = x, y = as.factor(y))
svmfit <- svm(y ~ ., data = dat, kernel = "linear", cost = .01,
scale = F)
plot(svmfit)
plot(svmfit, dat)
rm(list = ls())
set.seed(1)
x <- matrix(rnorm(20 * 2), ncol = 2)
y <- c(rep(-1, 10), rep(1, 10))
x[y == 1, ] <- x[y == 1, ] + 1
plot(x, col = (3 - y))
dat <- data.frame(x = x, y = as.factor(y))
library(e1071)
svmfit <- svm(y ~., data = dat, kernal = "linear", cost = .1,
scale = F)
plot(svmfit, dat)
plot(x, col = (3 - y))
x
svmfit <- svm(y ~., data = dat, kernal = "linear", cost = 10,
scale = F)
plot(svmfit, dat)
svmfit$index
svmfit <- svm(y ~., data = dat, kernal = "linear", cost = 0.1,
scale = F)
plot(svmfit, dat)
svmfit$index
svmfit <- svm(y ~., data = dat, kernal = "linear", cost = 0.1,
scale = F)
plot(svmfit, dat)
svmfit$index
set.seed(1)
tune.out <- tune(svm, y ~ ., data = dat, kernel = "linear",
ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tune.out)
bestmod = tune.out$best.model
summary(bestmod)
xtest = matrix(rnorm(20 * 2), ncol = 2)
ytest = sample(c(1, -1), 20, rep = T)
xtest[ytest == 1, ] = xtest[ytest == 1, ] + 1
testdat = data.frame(x = xtest, y = as.factor(ytest))
ypred <- predict(bestmod, testdat)
table(predict = ypred, truth = testdat$y)
svmfit <- svm(y ~ ., data = dat, kernel = "linear",
cost = 0.01, scale = F)
ypred <- predict(svmfit, testdat)
table(predict = ypred, truth = testdat$y)
x[y == 1, ] = x[y == 1, ] + 0.5
plot(x, col = (y + 5)/2, pch = 19)
dat <- data.frame(x = x, y = as.factor(y))
svmfit <- svm(y ~ ., data = dat, kernel = "linear", cost = 1e+05,
scale = F)
plot(svmfit, dat)
svmfit <- svm(y ~ ., data = dat, kernel = "linear", cost = 1)
summary(svmfit)
plot(svmfit, dat)
set.seed(1)
x = matrix(rnorm(200 * 2), ncol = 2)
x[1:100, ] = x[1:100, ] + 2
x[101:150, ] = x[101:150, ] - 2
y = c(rep(1, 150), rep(-1, 50))
dat = data.frame(x = x, y = as.factor(y))
plot(x, col = y)
plot(x, col = (3 - y))
train = sample(1:200, 100)
train
sum(train)
train = sample(200, 100)
train
svmfit <- svm(y ~., data = dat[train, ], kernel = "radial", gamma = 1,
cost = 1)
plot(svmfit, dat[train, ])
summary(svmfit)
svmfit <- svm(y ~., data = dat[train, ], kernel = "radial",
gamma = 1, cost = 1e5)
plot(svmfit, dat[train, ])
set.seed(1)
set.seed(1)
tune.out <- tune(svm, y ~., data = dat[train, ], kernel = "radial",
ranges = list(cost = c(0.1, 1, 10, 100, 1000)))
tune.out <- tune(svm, y ~., data = dat[train, ], kernel = "radial",
ranges = list(cost = c(0.1, 1, 10, 100, 1000)),
gamma = c(0.5, 1, 2, 3, 4))
summary(tune.out)
tune.out <- tune(svm, y ~., data = dat[train, ], kernel = "radial",
ranges = list(cost = c(0.1, 1, 10, 100, 1000),
gamma = c(0.5, 1, 2, 3, 4)))
summary(tune.out)
table(true = dat[-train, "y"], pred = predict(tune, out$best.model,
newx = dat[-train,]))
table(true = dat[-train, "y"], pred = predict(tune.out$best.model,
newx = dat[-train,]))
library(ROCR)
rocplot <- function(pred, truth, ...){
predob <- prediction(pred, truth)
perf <- performance(predob, "tpr", "fpr")
plot(perf, ...)
}
install.packages("ROCR")
svmfit.opt <- svm(y ~ ., data = dat[train, ], kernel = "radial",
gamma = 2, cost = 1, decision.values = T)
fitted <- attributes(predict(svmfit.opt, dat[train, ],
decision.values = T))$decision.values
par(mfrow = c(1, 2))
rocplot(fitted, dat[train, "y"], main = "training data")
library(ROCR)
rocplot(fitted, dat[train, "y"], main = "training data")
rm(list = ls())
set.seed(1)
x <- matrix(rnorm(20 * 2), ncol = 2)
x
y <- c(rep(1, 10), rep(-1, 10))
x[y == 1] <- x[y == 1] + 1
x
rm(list = ls())
set.seed(1)
x <- matrix(rnorm(20 * 2), ncol = 2)
y <- c(rep(1, 10), rep(-1, 10))
x[y == 1, ] <- x[y == 1, ] + 1
plot(x, col = (3 - y))
dat <- data.frame(x = x, y = as.factor(y))
dat
library(e1073)
library(e1071)
svmfit <- svm(y ~., data = dat, kernel = "linear", cost = 10,
scale = F)
summary(svmfit)
plot(svmfit, dat)
svmfit$index
svmfit <- svm(y ~., data = dat, kernel = "linear", cost = 0.1,
scale = F)
plot(svmfit, dat)
svmfit$vector
svmfit$index
tune.out <- tune(svm, y~., data = dat, kernel = "linear",
ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10)))
attribute(tune.out)
attributes(tune.out)
tune.out$best.model
bestmod <- tune.out$best.model
summary(bestmod)
xtest <- matrix(rnorm(200 * 2), ncol = 2)
ytest <- c(rep(1, 100), rep(-1, 100))
xtest[ytest == 1, ] <- xtest[ytest == 1, ] + 1
xtest <- matrix(rnorm(200 * 2), ncol = 2)
ytest <- c(rep(1, 100), rep(-1, 100))
xtest[ytest == 1, ] <- xtest[ytest == 1, ] + 1
head(xtest)
dat.test <- data.frame(x = xtest, y = as.factor(ytest))
plot(svmfit, dat.test)
ypred <- predict(bestmod, testdat)
ypred <- predict(bestmod, dat.test)
table(predict = ypred, truth = dat.test$y)
state <- row.names(USArrests)
state
rm(list = ls())
states <- row.names(USArrests)
states
names(USArrests)
apply(USArrests, 2, mean)
apply(USArrests, 2, var)
pr.out <- prcomp(USArrests, scale = T)
names(pr.out)
pr.out$center
apply(USArrests, 2, mean)
pr.out$scale
pr.out$rotation
pr.out$x
dim(pr.out$x)
biplot(pr.out, scale = 0)
pr.out$sdev
pr.var <- pr.out$sdev / sum(pr.out$sdev)
pr.var
pr.var <- pr.out$sdev ^ 2
pr.var <- pr.var / sum(pr.var)
pr.var
plot(pr.var, xlab = "Principle Component", ylab = "Proportion of
Variance Explained", ylim = c(0, 1), type = "b")
plot(cumsum(pve), xlab = "Principle Component", ylab = "Cumulative
Proportion of Variance Explained", ylim = c(0, 1), type = "b")
plot(cumsum(pr.var), xlab = "Principle Component", ylab = "Cumulative
Proportion of Variance Explained", ylim = c(0, 1), type = "b")
set.seed(2)
x <- matrix(rnorm(50 * 2), ncol = 2)
x[1:25, 1] <- x[1:25, 1] + 3
x[1:25, 2] <- x[1:25, 2] - 4
km.out <- kmeans(x, 2, nstart = 20)
km.out$cluster
plot(x, col = (km.out$cluster + 1), main = "k-means clustering",
xlab = "x1", ylab = "x2", pch = 20, cex = 2)
set.seed(4)
km.out <- kmeans(x, 3, nstart = 20)
?kmeans
km.out
plot(x, col = (km.out$cluster + 1), main = "k-means clustering with
3 cluster", pch = 20, cex = 2)
set.seed(3)
km.out <- kmeans(x, 3, nstart = 1)
km.out$tot.withinss
km.out <- kmeans(x, 3, nstart = 20)
km.out$tot.withinss
hc.complete <- hclust(dist(x), method = "complete")
hc.average <- hclust(dist(x), method = "average")
hc.single <- hclust(dist(x), method = "single")
cutree(hc.complete, 2)
cutree(hc.complete, 3)
cutree(hc.complete, 2)
cutree(hc.average, 2)
cutree(hc.single, 2)
xsc <- scale(x)
plot(hclust(dist(xsc), method = "complete"), main = "Hierarchical
Clustering with Scaled Features")
x <- matrix(rnorm(3 * 30), ncol = 3)
dd <- as.dist(1 - cor(t(x)))
plot(hclust(dd, method = "complete"), main = "Correlation based distance")
plot(hclust(dd, method = "complete"), main = "Correlation based distance")
library(ISLR)
nci.labs <- NCI60$labs
nci.data <- NCI60$data
dim(nci.data)
nci.labs[1:4]
table(nci.labs)
pr.out <- prcomp(nci.data, scale = T)
return(cols[as.numeric(as.factor(vec))])
Cols <- function(vec){
cols <- rainbow(length(unique(vec)))
return(cols[as.numeric(as.factor(vec))])
}
?rainbow
par(mfrow = c(1, 2))
plot(pr.out$x[, 1:2], col = Cols(nci.labs), pch = 19,
xlab = "Z1", ylab = "Z2")
plot(pr.out$X[, c(1, 3)], col = Cols(nci.labs), pch = 19,
xlab = "Z1", ylab = "Z3")
plot(pr.out$x[, c(1, 3)], col = Cols(nci.labs), pch = 19,
xlab = "Z1", ylab = "Z3")
par(mfrow = c(1, 2))
plot(pr.out$x[, 1:2], col = Cols(nci.labs), pch = 19,
xlab = "Z1", ylab = "Z2")
plot(pr.out$x[, c(1, 3)], col = Cols(nci.labs), pch = 19,
xlab = "Z1", ylab = "Z3")
summary(pr.out)
plot(pr.out)
pve <- 100 * pr.out$sdev ^ 2 / sum(pr.out$sdev ^ 2)
par(mfrow = c(1, 2))
plot(pve, type = "o", ylab = "PVE", xlab = "Principle Component",
col = "blue")
plot(cumsum(pve), type = "o", ylab = "cum PVE", xlab = "Principle Component",
col = "brown3")
sd.data <- scale(nci.data)
par(mfrow = c(1, 3))
data.dist <- dist(sd.data)
plot(hclust(data.dist), labels = nci.labs, main = "Complete Linkage",
xlab = "", sub = "", ylab = "")
plot(hclust(data.dist, method = "average"), labels = nci.labs, main = "Complete Linkage",
xlab = "", sub = "", ylab = "")
par(mfrow = c(1, 3))
data.dist <- dist(sd.data)
plot(hclust(data.dist), labels = nci.labs, main = "Complete Linkage",
xlab = "", sub = "", ylab = "")
plot(hclust(data.dist, method = "average"), labels = nci.labs, main = "Average Linkage",
xlab = "", sub = "", ylab = "")
plot(hclust(data.dist, method = "single"), labels = nci.labs, main = "Single Linkage",
xlab = "", sub = "", ylab = "")
hc.out <- hclust(dist(sd.data))
hc.clusters <- cutree(hc.out, 4)
table(hc.clusters, nci.labs)
par(mfrow = c(1, 1))
plot(hc.out, labels = nci.labs)
abline(h = 139, col = "red")
hc.out
set.seed(2)
km.out <- kmeans(sd.data, 4, nstart = 20)
km.clusters <- km.out$cluster
table(km.clusters, hc.clusters)
ls
ls()
